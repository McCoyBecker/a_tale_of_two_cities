@inproceedings{10.1145/3314221.3314642,
    author = {Cusumano-Towner, Marco F. and Saad, Feras A. and Lew, Alexander K. and Mansinghka, Vikash K.},
    title = {Gen: A General-Purpose Probabilistic Programming System with Programmable Inference},
    year = {2019},
    isbn = {9781450367127},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3314221.3314642},
    doi = {10.1145/3314221.3314642},
    booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
    pages = {221–236},
    numpages = {16},
    keywords = {variational inference, sequential Monte Carlo, Probabilistic programming, Markov chain Monte Carlo},
    location = {Phoenix, AZ, USA},
    series = {PLDI 2019}
}

@article{PRETNAR201519,
    title = "An Introduction to Algebraic Effects and Handlers. Invited tutorial paper",
    journal = "Electronic Notes in Theoretical Computer Science",
    volume = "319",
    pages = "19 - 35",
    year = "2015",
    note = "The 31st Conference on the Mathematical Foundations of Programming Semantics (MFPS XXXI).",
    issn = "1571-0661",
    doi = "https://doi.org/10.1016/j.entcs.2015.12.003",
    url = "http://www.sciencedirect.com/science/article/pii/S1571066115000705",
    author = "Matija Pretnar",
    keywords = "algebraic effects, handlers, effect system, semantics, logic, tutorial",
    abstract = "This paper is a tutorial on algebraic effects and handlers. In it, we explain what algebraic effects are, give ample examples to explain how handlers work, define an operational semantics and a type & effect system, show how one can reason about effects, and give pointers for further reading."
}

@article{williams1992simple,
    title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
    author = {Williams, Ronald J},
    journal = {Machine learning},
    volume = {8},
    number = {3-4},
    pages = {229--256},
    year = {1992},
    publisher = {Springer}
}

@article{fu2006gradient,
    title = {Gradient estimation},
    author = {Fu, Michael C},
    journal = {Handbooks in operations research and management science},
    volume = {13},
    pages = {575--616},
    year = {2006},
    publisher = {Elsevier}
}

@article{glynn1990likelihood,
    title = {Likelihood ratio gradient estimation for stochastic systems},
    author = {Glynn, Peter W},
    journal = {Communications of the ACM},
    volume = {33},
    number = {10},
    pages = {75--84},
    year = {1990},
    publisher = {ACM}
}

@misc{meent2018introduction,
    title={An Introduction to Probabilistic Programming},
    author={Jan-Willem van de Meent and Brooks Paige and Hongseok Yang and Frank Wood},
    year={2018},
    eprint={1809.10756},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@book{griewank2008evaluating,
    added-at = {2010-11-11T15:22:40.000+0100},
    address = {Philadelphia},
    author = {Griewank, Andreas and Walther, Andrea},
    biburl = {https://www.bibsonomy.org/bibtex/2847a29d144e86d5d509225016a7c4db0/toevanen},
    edition = {2nd},
    interhash = {819d56d9b2abf790fcaac18cede1e60f},
    intrahash = {847a29d144e86d5d509225016a7c4db0},
    keywords = {AD book},
    publisher = {SIAM},
    timestamp = {2011-10-25T15:17:57.000+0200},
    title = {Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation},
    year = 2008
}

@article{bartholomew-biggs_automatic_nodate,
    title = {Automatic differentiation of algorithms},
    abstract = {We introduce the basic notions of automatic di erentiation, describe some extensions which are of interest in the context of nonlinear optimization and give some illustrative examples. c 2000 Elsevier Science B.V. All rights reserved.},
    language = {en},
    author = {Bartholomew-Biggs, Michael and Brown, Steven and Christianson, Bruce and Dixon, Laurence},
    pages = {20},
    file = {Bartholomew-Biggs et al. - Automatic di erentiation of algorithms.pdf:/home/mccoy/Zotero/storage/MFLHAMYY/Bartholomew-Biggs et al. - Automatic di erentiation of algorithms.pdf:application/pdf}
}

@article{van_merrienboer_automatic_nodate,
    title = {Automatic differentiation in {ML}: {Where} we are and where we should be going},
    abstract = {We review the current state of automatic differentiation (AD) for array programming in machine learning (ML), including the different approaches such as operator overloading (OO) and source transformation (ST) used for AD, graph-based intermediate representations for programs, and source languages. Based on these insights, we introduce a new graph-based intermediate representation (IR) which speciﬁcally aims to efﬁciently support fully-general AD for array programming. Unlike existing dataﬂow programming representations in ML frameworks, our IR naturally supports function calls, higher-order functions and recursion, making ML models easier to implement. The ability to represent closures allows us to perform AD using ST without a tape, making the resulting derivative (adjoint) program amenable to ahead-of-time optimization using tools from functional language compilers, and enabling higher-order derivatives. Lastly, we introduce a proof of concept compiler toolchain called Myia which uses a subset of Python as a front end.},
    language = {en},
    author = {van Merriënboer, Bart and Breuleux, Olivier and Bergeron, Arnaud and Lamblin, Pascal},
    pages = {11},
    file = {van Merriënboer et al. - Automatic differentiation in ML Where we are and .pdf:/home/mccoy/Zotero/storage/WVCVS6IA/van Merriënboer et al. - Automatic differentiation in ML Where we are and .pdf:application/pdf}
}

@article{baydin_automatic_nodate,
    title = {Automatic {Diﬀerentiation} in {Machine} {Learning}: a {Survey}},
    abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic diﬀerentiation (AD), also called algorithmic diﬀerentiation or simply “autodiﬀ”, is a family of techniques similar to but more general than backpropagation for eﬃciently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established ﬁeld with applications in areas including computational ﬂuid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the ﬁelds of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other’s results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names “dynamic computational graphs” and “diﬀerentiable programming”. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely deﬁning the main diﬀerentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms “autodiﬀ”, “automatic diﬀerentiation”, and “symbolic diﬀerentiation” as these are encountered more and more in machine learning settings.},
    language = {en},
    author = {Baydin, Atılım Gunes and Pearlmutter, Barak A and Radul, Alexey Andreyevich and Siskind, Jeﬀrey Mark},
    pages = {43},
    file = {Baydin et al. - Automatic Diﬀerentiation in Machine Learning a Su.pdf:/home/mccoy/Zotero/storage/W3PUV2EN/Baydin et al. - Automatic Diﬀerentiation in MachineLearning a Su.pdf:application/pdf}
}

@article{paszke_automatic_nodate,
    title = {Automatic differentiation in {PyTorch}},
    abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
    language = {en},
    author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
    pages = {4},
    file = {Paszke et al. - Automatic differentiation in PyTorch.pdf:/home/mccoy/Zotero/storage/GICUKG4V/Paszke et al. - Automatic differentiation in PyTorch.pdf:application/pdf}
}


@article{elliott_simple_2018,
    title = {The simple essence of automatic differentiation},
    volume = {2},
    issn = {24751421},
    url = {http://dl.acm.org/citation.cfm?doid=3243631.3236765},
    doi = {10.1145/3236765},
    language = {en},
    number = {ICFP},
    urldate = {2019-10-14},
    journal = {Proceedings of the ACM on Programming Languages},
    author = {Elliott, Conal},
    month = jul,
    year = {2018},
    pages = {1--29},
    file = {Elliott - 2018 - The simple essence of automatic differentiation.pdf:/home/mccoy/Zotero/storage/KQFNDDQS/Elliott - 2018 - The simple essence of automatic differentiation.pdf:application/pdf}
}


@article{wingate_lightweight_nodate,
    title = {Lightweight {Implementations} of {Probabilistic} {Programming} {Languages} {Via} {Transformational} {Compilation}},
    abstract = {We describe a general method of transforming arbitrary programming languages into probabilistic programming languages with straightforward MCMC inference engines. Random choices in the program are “named” with information about their position in an execution trace; these names are used in conjunction with a database holding values of random variables to implement MCMC inference in the space of execution traces. We encode naming information using lightweight source-to-source compilers. Our method enables us to reuse existing infrastructure (compilers, proﬁlers, etc.) with minimal additional code, implying fast models with low development overhead. We illustrate the technique on two languages, one functional and one imperative: Bher, a compiled version of the Church language which eliminates interpretive overhead of the original MIT-Church implementation, and Stochastic Matlab, a new open-source language.},
    language = {en},
    author = {Wingate, David and Stuhlmüller, Andreas and Goodman, Noah D},
    pages = {9},
    file = {Wingate et al. - Lightweight Implementations of Probabilistic Progr.pdf:/home/mccoy/Zotero/storage/H6X28MGC/Wingate et al. - Lightweight Implementations of Probabilistic Progr.pdf:application/pdf}
}


@article{cusumano-towner_using_2018,
    title = {Using probabilistic programs as proposals},
    url = {http://arxiv.org/abs/1801.03612},
    abstract = {Monte Carlo inference has asymptotic guarantees, but can be slow when using generic proposals. Handcrafted proposals that rely on user knowledge about the posterior distribution can be efﬁcient, but are difﬁcult to derive and implement. This paper proposes to let users express their posterior knowledge in the form of proposal programs, which are samplers written in probabilistic programming languages. One strategy for writing good proposal programs is to combine domain-speciﬁc heuristic algorithms with neural network models. The heuristics identify high probability regions, and the neural networks model the posterior uncertainty around the outputs of the algorithm. Proposal programs can be used as proposal distributions in importance sampling and Metropolis-Hastings samplers without sacriﬁcing asymptotic consistency, and can be optimized ofﬂine using inference compilation. Support for optimizing and using proposal programs is easily implemented in a sampling-based probabilistic programming runtime. The paper illustrates the proposed technique with a proposal program that combines RANSAC and neural networks to accelerate inference in a Bayesian linear regression with outliers model.},
    language = {en},
    urldate = {2019-11-30},
    journal = {arXiv:1801.03612 [cs, stat]},
    author = {Cusumano-Towner, Marco F. and Mansinghka, Vikash K.},
    month = jan,
    year = {2018},
    note = {arXiv: 1801.03612},
    keywords = {Computer Science - Artificial Intelligence, Statistics - Computation},
    file = {Cusumano-Towner and Mansinghka - 2018 - Using probabilistic programs as proposals.pdf:/home/mccoy/Zotero/storage/R3JX4U6T/Cusumano-Towner and Mansinghka - 2018 - Using probabilistic programs as proposals.pdf:application/pdf}
}


@article{wainwright_graphical_2007,
    title = {Graphical {Models}, {Exponential} {Families}, and {Variational} {Inference}},
    volume = {1},
    issn = {1935-8237, 1935-8245},
    url = {http://www.nowpublishers.com/article/Details/MAL-001},
    doi = {10.1561/2200000001},
    language = {en},
    number = {1–2},
    urldate = {2020-04-24},
    journal = {Foundations and Trends® in Machine Learning},
    author = {Wainwright, Martin J. and Jordan, Michael I.},
    year = {2007},
    pages = {1--305},
    file = {Wainwright and Jordan - 2007 - Graphical Models, Exponential Families, and Variat.pdf:/home/mccoy/Zotero/storage/2NLZIHK7/Wainwright and Jordan - 2007 - Graphical Models, Exponential Families, and Variat.pdf:application/pdf}
}

